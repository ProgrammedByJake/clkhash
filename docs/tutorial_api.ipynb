{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Tutorial for Python API\n",
    "\n",
    "For this tutorial we are going to process a data set for private linkage with clkhash using the Python API. Note you can also use the command line tool.\n",
    "\n",
    "The Python package `recordlinkage` has a [tutorial](http://recordlinkage.readthedocs.io/en/latest/notebooks/link_two_dataframes.html) linking data sets in the clear, we will try duplicate that in a privacy preserving setting.\n",
    "\n",
    "First install clkhash, recordlinkage and a few data science tools (pandas and numpy):\n",
    "\n",
    "    $ pip install -U clkhash anonlink recordlinkage numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import clkhash\n",
    "from clkhash import clk\n",
    "from clkhash.field_formats import *\n",
    "from clkhash.schema import Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import recordlinkage\n",
    "from recordlinkage.datasets import load_febrl4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Data Exploration\n",
    "\n",
    "First we have a look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "dfA, dfB = load_febrl4()\n",
    "\n",
    "dfA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "For this linkage we will **not** use the social security id column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "dfA.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "a_csv = io.StringIO()\n",
    "dfA.to_csv(a_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Hashing Schema Definition\n",
    "\n",
    "A hashing schema instructs clkhash how to treat each column for generating CLKs. A detailed description of the hashing schema can be found in the [api docs](http://clkhash.readthedocs.io/en/latest/schema.html). We will ignore the columns 'rec_id' and 'soc_sec_id' for CLK generation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "fields = [\n",
    "    Ignore('rec_id'),\n",
    "    StringSpec('given_name', FieldHashingProperties(ngram=2, num_bits=300)),\n",
    "    StringSpec('surname', FieldHashingProperties(ngram=2, num_bits=300)),\n",
    "    IntegerSpec('street_number', FieldHashingProperties(ngram=1, positional=True, num_bits=300, missing_value=MissingValueSpec(sentinel=''))),\n",
    "    StringSpec('address_1', FieldHashingProperties(ngram=2, num_bits=300)),\n",
    "    StringSpec('address_2', FieldHashingProperties(ngram=2, num_bits=300)),\n",
    "    StringSpec('suburb', FieldHashingProperties(ngram=2, num_bits=300)),\n",
    "    IntegerSpec('postcode', FieldHashingProperties(ngram=1, positional=True, num_bits=300)),\n",
    "    StringSpec('state', FieldHashingProperties(ngram=2, num_bits=300)),\n",
    "    IntegerSpec('date_of_birth', FieldHashingProperties(ngram=1, positional=True, num_bits=300, missing_value=MissingValueSpec(sentinel=''))),\n",
    "    Ignore('soc_sec_id')\n",
    "]\n",
    "\n",
    "schema = Schema(fields, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Hash the data\n",
    "\n",
    "We can now hash our PII data from the CSV file using our defined schema. We must provide a list of *secret keys* to this command - these keys have to be used by both parties hashing data. For this toy example we will use the keys _'key1'_ and _'key2'_, for real data, make sure that the keys contain enough entropy, as knowledge of these keys is sufficient to reconstruct the PII information from a CLK! \n",
    "\n",
    "Also, **do not share these keys with anyone, except the other participating party.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "secret_keys = ('key1', 'key2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "a_csv.seek(0)\n",
    "hashed_data_a = clk.generate_clk_from_csv(a_csv, secret_keys, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Inspect the output\n",
    "\n",
    "clkhash has hashed the PII, creating a Cryptographic Longterm Key for each entity. The output of `generate_clk_from_csv` shows that the mean popcount is quite high (950 out of 1024) which can affect accuracy.\n",
    "\n",
    "We can control the popcount by adjusting the hashing strategy. There are currently two different strategies implemented in the library.\n",
    "- _fixed k_: each n-gram of a feature's value is inserted into the CLK *k* times. Increasing *k* will give the corresponding feature more importance in comparisons, decreasing *k* will de-emphasise columns which are less suitable for linkage (e.g. information that changes frequently). The _fixed k_ strategy is set with the 'k=30' argument for each feature's FieldHashingProperties. (for a total of numberOfTokens * k insertions)\n",
    "- _fixed number of bits_: In this strategy we always insert a fixed number of bits into the CLK for a feature, irrespective of the number of n-grams. This strategy is set with the 'numBits=100' argument for each feature's FieldHashingProperties.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "In this example, we will reduce the value of `num_bits` for address related columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "fields = [\n",
    "    Ignore('rec_id'),\n",
    "    StringSpec('given_name', FieldHashingProperties(ngram=2, num_bits=200)),\n",
    "    StringSpec('surname', FieldHashingProperties(ngram=2, num_bits=200)),\n",
    "    IntegerSpec('street_number', FieldHashingProperties(ngram=1, positional=True, num_bits=100, missing_value=MissingValueSpec(sentinel=''))),\n",
    "    StringSpec('address_1', FieldHashingProperties(ngram=2, num_bits=100)),\n",
    "    StringSpec('address_2', FieldHashingProperties(ngram=2, num_bits=100)),\n",
    "    StringSpec('suburb', FieldHashingProperties(ngram=2, num_bits=100)),\n",
    "    IntegerSpec('postcode', FieldHashingProperties(ngram=1, positional=True, num_bits=100)),\n",
    "    StringSpec('state', FieldHashingProperties(ngram=2, num_bits=100)),\n",
    "    IntegerSpec('date_of_birth', FieldHashingProperties(ngram=1, positional=True, num_bits=200, missing_value=MissingValueSpec(sentinel=''))),\n",
    "    Ignore('soc_sec_id')\n",
    "]\n",
    "\n",
    "schema = Schema(fields, 1024)\n",
    "a_csv.seek(0)\n",
    "hashed_data_a = clk.generate_clk_from_csv(a_csv, secret_keys, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Each CLK is serialized in a JSON friendly base64 format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "hashed_data_a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Hash data set B\n",
    "\n",
    "Now we hash the second dataset using the same keys and same schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "b_csv = io.StringIO()\n",
    "dfB.to_csv(b_csv)\n",
    "b_csv.seek(0)\n",
    "hashed_data_b = clkhash.clk.generate_clk_from_csv(b_csv, secret_keys, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "len(hashed_data_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Find matches between the two sets of CLKs\n",
    "\n",
    "We have generated two sets of CLKs which represent entity information in a privacy-preserving way. The more similar two CLKs are, the more likely it is that they represent the same entity.\n",
    "\n",
    "For this task we will use [anonlink](https://github.com/data61/anonlink), a Python (and optimised C++) implementation of anonymous linkage using CLKs. \n",
    "\n",
    "As the CLKs are in a string format we first deserialize to use the bitarray type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from bitarray import bitarray\n",
    "import base64\n",
    "\n",
    "def deserialize_bitarray(bytes_data):\n",
    "    ba = bitarray(endian='big')\n",
    "    data_as_bytes = base64.decodebytes(bytes_data.encode())\n",
    "    ba.frombytes(data_as_bytes)\n",
    "    return ba\n",
    "\n",
    "def deserialize_filters(filters):\n",
    "    res = []\n",
    "    for i, f in enumerate(filters):\n",
    "        ba = deserialize_bitarray(f)\n",
    "        res.append(ba)\n",
    "    return res\n",
    "\n",
    "clks_a = deserialize_filters(hashed_data_a)\n",
    "clks_b = deserialize_filters(hashed_data_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Using `anonlink` we find the candidate pairs - which is all possible pairs above the given `threshold`. Then we solve for the most likely mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import anonlink\n",
    "\n",
    "def mapping_from_clks(clks_a, clks_b, threshold):\n",
    "    results_candidate_pairs = anonlink.candidate_generation.find_candidate_pairs(\n",
    "            [clks_a, clks_b],\n",
    "            anonlink.similarities.dice_coefficient,\n",
    "            threshold\n",
    "    )\n",
    "    solution = anonlink.solving.greedy_solve(results_candidate_pairs)\n",
    "    print('Found {} matches'.format(len(solution)))\n",
    "    return {a:b for ((_, a),(_, b)) in solution}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "mapping = mapping_from_clks(clks_a, clks_b, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Let's investigate some of those matches and the overall matching quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "a_csv.seek(0)\n",
    "b_csv.seek(0)\n",
    "a_raw = a_csv.readlines()\n",
    "b_raw = b_csv.readlines()\n",
    "\n",
    "num_entities = len(b_raw) - 1\n",
    "  \n",
    "def describe_accuracy(mapping, show_examples=False):\n",
    "    if show_examples:\n",
    "        print('idx_a, idx_b,     rec_id_a,       rec_id_b')\n",
    "        print('---------------------------------------------')\n",
    "        for a_i in range(10):\n",
    "            if a_i in mapping:\n",
    "                a_data = a_raw[a_i + 1].split(',')\n",
    "                b_data = b_raw[mapping[a_i] + 1].split(',')\n",
    "                print('{:3}, {:6}, {:>15}, {:>15}'.format(a_i+1, mapping[a_i]+1, a_data[0], b_data[0]))\n",
    "        print('---------------------------------------------')\n",
    "    \n",
    "    TP = 0; FP = 0; TN = 0; FN = 0\n",
    "    for a_i in range(num_entities):\n",
    "        if a_i in mapping:\n",
    "            if a_raw[a_i + 1].split(',')[0].split('-')[1] == b_raw[mapping[a_i] + 1].split(',')[0].split('-')[1]:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "                # as we only report one mapping for each element in PII_a, \n",
    "                # then a wrong mapping is not only a false positive, but \n",
    "                # also a false negative, as we won't report the true mapping.\n",
    "                FN += 1 \n",
    "        else:\n",
    "            FN += 1 # every element in PII_a has a partner in PII_b\n",
    "\n",
    "    print()\n",
    "    print(\"We've got {} true positives, {} false positives, and {} false negatives.\".format(TP, FP, FN))\n",
    "    print('Precision: {:.3f}, Recall: {:.3f}, Accuracy: {:.3f}'.format(\n",
    "        TP/(TP+FP), \n",
    "        TP/(TP+FN), \n",
    "        (TP+TN)/(TP+TN+FP+FN)))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "describe_accuracy(mapping, show_examples=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Precision tells us about how many of the found matches are actual matches. The score of 1.0 means that we did perfectly in this respect, however, recall, the measure of how many of the actual matches were correctly identified, is quite low with only 80%.\n",
    "\n",
    "Let's go back to the mapping calculation (`calculate_mapping_greedy`) an reduce the value for `threshold` to `0.8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "mapping = mapping_from_clks(clks_a, clks_b, 0.8)\n",
    "describe_accuracy(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Great, for this threshold value we get a precision of 100% and a recall of 99.6%. \n",
    "\n",
    "The explanation is that when the information about an entity differs slightly in the two datasets (e.g. spelling errors, abbrevations, missing values, ...) then the corresponding CLKs will differ in some number of bits as well. It is important to choose an appropriate threshold for the amount of perturbations present in the data (a threshold of 0.74 and below generates a mapping which misses just one true match).\n",
    "\n",
    "This concludes the tutorial. Feel free to go back to the CLK generation and experiment on how different setting will affect the matching quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
